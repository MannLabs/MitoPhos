{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> map_UniProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import zlib\n",
    "from xml.etree import ElementTree\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "POLLING_INTERVAL = 3\n",
    "\n",
    "API_URL = \"https://rest.uniprot.org\"\n",
    "\n",
    "\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "\n",
    "def submit_id_mapping(from_db, to_db, ids):\n",
    "    request = requests.post(\n",
    "        f\"{API_URL}/idmapping/run\",\n",
    "        data={\"from\": from_db, \"to\": to_db, \"ids\": \",\".join(ids)},\n",
    "    )\n",
    "    request.raise_for_status()\n",
    "    return request.json()[\"jobId\"]\n",
    "\n",
    "def get_next_link(headers):\n",
    "    re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "\n",
    "def check_id_mapping_results_ready(job_id):\n",
    "    while True:\n",
    "        request = session.get(f\"{API_URL}/idmapping/status/{job_id}\")\n",
    "        request.raise_for_status()\n",
    "        j = request.json()\n",
    "        if \"jobStatus\" in j:\n",
    "            if j[\"jobStatus\"] == \"RUNNING\":\n",
    "                print(f\"Retrying in {POLLING_INTERVAL}s\")\n",
    "                time.sleep(POLLING_INTERVAL)\n",
    "            else:\n",
    "                raise Exception(request[\"jobStatus\"])\n",
    "        else:\n",
    "            return bool(j[\"results\"] or j[\"failedIds\"])\n",
    "\n",
    "\n",
    "def get_batch(batch_response, file_format, compressed):\n",
    "    batch_url = get_next_link(batch_response.headers)\n",
    "    while batch_url:\n",
    "        batch_response = session.get(batch_url)\n",
    "        batch_response.raise_for_status()\n",
    "        yield decode_results(batch_response, file_format, compressed)\n",
    "        batch_url = get_next_link(batch_response.headers)\n",
    "\n",
    "\n",
    "def combine_batches(all_results, batch_results, file_format):\n",
    "    if file_format == \"json\":\n",
    "        for key in (\"results\", \"failedIds\"):\n",
    "            if key in batch_results and batch_results[key]:\n",
    "                all_results[key] += batch_results[key]\n",
    "    elif file_format == \"tsv\":\n",
    "        return all_results + batch_results[1:]\n",
    "    else:\n",
    "        return all_results + batch_results\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_link(job_id):\n",
    "    url = f\"{API_URL}/idmapping/details/{job_id}\"\n",
    "    request = session.get(url)\n",
    "    request.raise_for_status()\n",
    "    return request.json()[\"redirectURL\"]\n",
    "\n",
    "\n",
    "def decode_results(response, file_format, compressed):\n",
    "    if compressed:\n",
    "        decompressed = zlib.decompress(response.content, 16 + zlib.MAX_WBITS)\n",
    "        if file_format == \"json\":\n",
    "            j = json.loads(decompressed.decode(\"utf-8\"))\n",
    "            return j\n",
    "        elif file_format == \"tsv\":\n",
    "            return [line for line in decompressed.decode(\"utf-8\").split(\"\\n\") if line]\n",
    "        elif file_format == \"xlsx\":\n",
    "            return [decompressed]\n",
    "        elif file_format == \"xml\":\n",
    "            return [decompressed.decode(\"utf-8\")]\n",
    "        else:\n",
    "            return decompressed.decode(\"utf-8\")\n",
    "    elif file_format == \"json\":\n",
    "        return response.json()\n",
    "    elif file_format == \"tsv\":\n",
    "        return [line for line in response.text.split(\"\\n\") if line]\n",
    "    elif file_format == \"xlsx\":\n",
    "        return [response.content]\n",
    "    elif file_format == \"xml\":\n",
    "        return [response.text]\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def get_xml_namespace(element):\n",
    "    m = re.match(r\"\\{(.*)\\}\", element.tag)\n",
    "    return m.groups()[0] if m else \"\"\n",
    "\n",
    "\n",
    "def merge_xml_results(xml_results):\n",
    "    merged_root = ElementTree.fromstring(xml_results[0])\n",
    "    for result in xml_results[1:]:\n",
    "        root = ElementTree.fromstring(result)\n",
    "        for child in root.findall(\"{http://uniprot.org/uniprot}entry\"):\n",
    "            merged_root.insert(-1, child)\n",
    "    ElementTree.register_namespace(\"\", get_xml_namespace(merged_root[0]))\n",
    "    return ElementTree.tostring(merged_root, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "\n",
    "def print_progress_batches(batch_index, size, total):\n",
    "    n_fetched = min((batch_index + 1) * size, total)\n",
    "    #print(f\"Fetched: {n_fetched} / {total}\")\n",
    "\n",
    "\n",
    "def get_id_mapping_results_search(url):\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    if \"size\" in query:\n",
    "        size = int(query[\"size\"][0])\n",
    "    else:\n",
    "        size = 500\n",
    "        query[\"size\"] = size\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    parsed = parsed._replace(query=urlencode(query, doseq=True))\n",
    "    url = parsed.geturl()\n",
    "    request = session.get(url)\n",
    "    request.raise_for_status()\n",
    "    results = decode_results(request, file_format, compressed)\n",
    "    total = int(request.headers[\"x-total-results\"])\n",
    "    print_progress_batches(0, size, total)\n",
    "    for i, batch in enumerate(get_batch(request, file_format, compressed), 1):\n",
    "        results = combine_batches(results, batch, file_format)\n",
    "        print_progress_batches(i, size, total)\n",
    "    if file_format == \"xml\":\n",
    "        return merge_xml_results(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_stream(url):\n",
    "    if \"/stream/\" not in url:\n",
    "        url = url.replace(\"/results/\", \"/stream/\")\n",
    "    request = session.get(url)\n",
    "    request.raise_for_status()\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    return decode_results(request, file_format, compressed)\n",
    "\n",
    "\n",
    "def map_UniProtID(UniProtIDs, identifier_from, identifier_to):\n",
    "    try:\n",
    "        job_id = submit_id_mapping(\n",
    "            from_db=identifier_from, to_db=identifier_to, ids=UniProtIDs \n",
    "        )\n",
    "        if check_id_mapping_results_ready(job_id):\n",
    "            link = get_id_mapping_results_link(job_id)\n",
    "            results = get_id_mapping_results_search(link)\n",
    "            # Equivalently using the stream endpoint which is more demanding\n",
    "            # on the API and so is less stable:\n",
    "            # results = get_id_mapping_results_stream(link)\n",
    "        \n",
    "    except:\n",
    "        ensemble_flat = []\n",
    "\n",
    "        \n",
    "    id_dict = {results['results'][i]['from']:results['results'][i]['to'] for i in range(len(results['results']))}\n",
    "    return id_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>get_ESGN_PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ESNG_PTM(df):\n",
    "    import urllib.parse\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    \n",
    "    url = 'https://www.uniprot.org/uploadlists/'\n",
    "    \n",
    "    UniProtID_new = []\n",
    "    ESNG_new = []\n",
    "    query = ''\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for x in range(len(df['Proteins'][i].split(';'))):\n",
    "            query = query + str(df['Proteins'][i].split(';')[x])+' '\n",
    "      \n",
    "    params = {\n",
    "    'from': 'ACC+ID',\n",
    "    'to': 'ENSEMBL_ID',\n",
    "    'format': 'tab',\n",
    "    'query': query\n",
    "    }   \n",
    "         \n",
    "    data = urllib.parse.urlencode(params)\n",
    "    data = data.encode('utf-8')\n",
    "    req = urllib.request.Request(url, data)\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        response = f.read()\n",
    "               \n",
    "    anno = str(response.decode('utf-8')).split()\n",
    "    \n",
    "    anno.remove('To')\n",
    "    anno.remove('From')\n",
    "    ESNG = []\n",
    "    UniProtID = []\n",
    "    for i in range(len(anno)):\n",
    "        if i % 2 == 0:\n",
    "            UniProtID.append(anno[i])\n",
    "            \n",
    "        else:\n",
    "            ESNG.append(anno[i])\n",
    "    \n",
    "    UniProtID_new.append(UniProtID)\n",
    "    ESNG_new.append(ESNG)\n",
    "    \n",
    "    ESNG = []\n",
    "    for i in range(len(df)):\n",
    "        y=[]\n",
    "        for x in range(len(df['Proteins'][i].split(';'))):\n",
    "            if df['Proteins'][i].split(';')[x] in UniProtID_new[0]:\n",
    "                index = UniProtID_new[0].index(df['Proteins'][i].split(';')[x])\n",
    "                y.append(ESNG_new[0][index])\n",
    "            else:\n",
    "                y.append('')\n",
    "        ESNG.append(y)\n",
    "    df['ESNG'] = ESNG \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>get_Entrez_PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_Entrez_PTM(df):\n",
    "    import urllib.parse\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    \n",
    "    url = 'https://www.uniprot.org/uploadlists/'\n",
    "    \n",
    "    UniProtID_new = []\n",
    "    Entrez_GeneID_new = []\n",
    "    query = ''\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for x in range(len(df['Proteins'][i].split(';'))):\n",
    "            query = query + str(df['Proteins'][i].split(';')[x])+' '\n",
    "      \n",
    "    params = {\n",
    "    'from': 'ACC+ID',\n",
    "    'to': '\tP_ENTREZGENEID',\n",
    "    'format': 'tab',\n",
    "    'query': query\n",
    "    }   \n",
    "         \n",
    "    data = urllib.parse.urlencode(params)\n",
    "    data = data.encode('utf-8')\n",
    "    req = urllib.request.Request(url, data)\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        response = f.read()\n",
    "               \n",
    "    anno = str(response.decode('utf-8')).split()\n",
    "    \n",
    "    anno.remove('To')\n",
    "    anno.remove('From')\n",
    "    Entrez_GeneID = []\n",
    "    UniProtID = []\n",
    "    for i in range(len(anno)):\n",
    "        if i % 2 == 0:\n",
    "            UniProtID.append(anno[i])\n",
    "            \n",
    "        else:\n",
    "            Entrez_GeneID.append(anno[i])\n",
    "    \n",
    "    UniProtID_new.append(UniProtID)\n",
    "    Entrez_GeneID_new.append(Entrez_GeneID)\n",
    "    \n",
    "    Entrez_GeneID = []\n",
    "    for i in range(len(df)):\n",
    "        y=[]\n",
    "        for x in range(len(df['Proteins'][i].split(';'))):\n",
    "            if df['Proteins'][i].split(';')[x] in UniProtID_new[0]:\n",
    "                index = UniProtID_new[0].index(df['Proteins'][i].split(';')[x])\n",
    "                y.append(Entrez_GeneID_new[0][index])\n",
    "            else:\n",
    "                y.append('')\n",
    "        Entrez_GeneID.append(y)\n",
    "    df['Entrez_GeneID'] = Entrez_GeneID \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>get_IMPI_annotation_PTM </h1> -->file import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IMPI_annotation_PTM(df):\n",
    "    path_IMPI = os.path.dirname(os.path.abspath('__file__'))\n",
    "    path_IMPI_file = path_IMPI+'\\\\IMPI_2021_Q4pre_Mus_Musculus.csv'\n",
    "    df_IMPI = pd.read_csv(path_IMPI_file)\n",
    "   #path_IMPI_file = 'Z:/Fynn/005_Mito/Reanalysis/IMPI_tables/IMPI_2020_Q3pre_Mus_Musculus.csv'\n",
    "   #df_IMPI = pd.read_csv(path_IMPI_file)\n",
    "    \n",
    "    \n",
    "    IMPI_new = []\n",
    "    for x in range(len(df)):\n",
    "        IMPI = []\n",
    "        for i in df['ESNG'][x]:\n",
    "            if i in list(df_IMPI['Ensembl Gene ID Mus Musculus']):\n",
    "                IMPI.append(list(df_IMPI[df_IMPI['Ensembl Gene ID Mus Musculus']==i]['IMPI Class'])[0])\n",
    "            else:\n",
    "                IMPI.append('NA')\n",
    "        IMPI_new.append(IMPI)\n",
    "        \n",
    "    df['IMPI_new']=IMPI_new\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>get_MitoCharta_annotation</h1> --> file import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MitoCharta_annotation(df):\n",
    "    path_MC3 = os.path.dirname(os.path.abspath('__file__'))\n",
    "    df_MC3 = pd.read_excel(path_MC3+'\\\\Mouse_MitoCarta3_0.xls', sheet_name = [0,1,2])\n",
    "    \n",
    "    SubMitoLocalization_new = []\n",
    "    Pathways_new = []\n",
    "    for x in range(len(df)):\n",
    "        SubMitoLocalization = []\n",
    "        Pathways = []\n",
    "        for i in df['Entrez_GeneID'][x]:\n",
    "            if i != '' and int(i) in list(df_MC3[1]['MouseGeneID']):\n",
    "                SubMitoLocalization.append(list(df_MC3[1][df_MC3[1]['MouseGeneID']==int(i)]['MitoCarta3.0_SubMitoLocalization'])[0])\n",
    "        SubMitoLocalization_new.append(list(set(SubMitoLocalization)))\n",
    "        \n",
    "        for i in df['Entrez_GeneID'][x]:\n",
    "            if i != '' and len(list(df_MC3[1][df_MC3[1]['MouseGeneID']==int(i)]['MitoCarta3.0_MitoPathways']))>0 and list(df_MC3[1][df_MC3[1]['MouseGeneID']==int(i)]['MitoCarta3.0_MitoPathways'])!=[0]:\n",
    "                Pathways.extend(list(df_MC3[1][df_MC3[1]['MouseGeneID']==int(i)]['MitoCarta3.0_MitoPathways'])[0].split('|'))\n",
    "        Pathways_new.append(list(set(Pathways)))\n",
    "            \n",
    "    df['SubMitoLocalization'] = SubMitoLocalization_new\n",
    "    df['Pathways'] = Pathways_new\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>filter_Mito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_Mito(df):\n",
    "    #df['IMPI_new']= [df['IMPI_new'][i].split(',') for i in range(len(df))]\n",
    "    row_filter = [i for i in range(len(df)) if (('Verified mitochondrial' in df['IMPI_new'][i])==True or \n",
    "                                                         #('Predicted mitochondrial' in df['IMPI_new'][i])==True or \n",
    "                                                         df['SubMitoLocalization'][i]!=[])]\n",
    "    df_filtered = df.iloc[row_filter,:].reset_index(drop=True)\n",
    "    return(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>filter_PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_PTM(df,samples=['B','BAT','H','K','L','S','SKM']):\n",
    "\n",
    "    valid= []\n",
    "    #get groups \n",
    "    for s in range(len(samples)):\n",
    "        col = []\n",
    "        per_val_val = []\n",
    "        values_total = []\n",
    "        [col.append(i) for i in range(len(list(df.columns))) if ((df.columns[i].find('Intensity wt')==0)&(df.columns[i].find('_'+samples[s])==(len(df.columns[i])-len(samples[s])-1)))==True]\n",
    "        df.iloc[:,col] = np.log2((df.iloc[:,col]).replace(0,np.nan))\n",
    "        for z in range(len(df)):\n",
    "            valid_values = 0\n",
    "            valid_values = [valid_values+1 for i in range(len(col)) if ((df.iloc[z,col[i]]>0) & (df.iloc[z,col[i]]!=np.nan))]\n",
    "            per_val_val.append(sum(valid_values)/len(col))\n",
    "            values_total.append(len(col))\n",
    "        df['Valid values_'+samples[s]] = per_val_val\n",
    "        df['Values_Tissue_'+samples[s]] = values_total\n",
    "\n",
    "    df['TissueID'] = df[['Valid values_'+samples[0],'Valid values_'+samples[1],'Valid values_'+samples[2],'Valid values_'+samples[3],'Valid values_'+samples[4],'Valid values_'+samples[5],'Valid values_'+samples[6]]].replace(0,np.nan).count(axis=1)\n",
    "    \n",
    "        \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    #calculate valid values    \n",
    "    col_val = []\n",
    "    for i in range(len(list(df.columns))):\n",
    "        if ((df.columns[i].find('Valid values_')==0)):\n",
    "            col_val.append(i)\n",
    "\n",
    "#    #get max valid value value across groups -> at least XX valid values in at least 1 group    \n",
    "#    for i in range(len(df)):\n",
    "#        if df.iloc[i,col_val].max()>=cutoff:\n",
    "#            valid.append(True)\n",
    "#        else:\n",
    "#            valid.append(False)\n",
    "#    df['valid'] = valid\n",
    "#    df = df[df['valid']==True]\n",
    "#    df = df.reset_index(drop=True)\n",
    "#\n",
    "    #Median normalization -> Substract Median log2 value of column from each member of column\n",
    "    cols = []\n",
    "    for i in range(len(list(df.columns))):\n",
    "        if df.columns[i].find('Intensity wt')==0:\n",
    "            cols.append(i)\n",
    "    \n",
    "    df = df.join(pd.DataFrame((df.iloc[:,cols]-df.iloc[:,cols].median()).to_numpy(), columns = list('Norm_'+df.columns[cols])))\n",
    "    \n",
    "    #z-Score across all expandet samples\n",
    "    cols = []\n",
    "    for i in range(len(list(df.columns))):\n",
    "        if df.columns[i].find('Norm_')==0:\n",
    "            cols.append(i)\n",
    "    \n",
    "    #df = df.join(pd.DataFrame(stats.zscore(df.iloc[:,cols], nan_policy = 'omit'), columns = list('Zscore_'+df.columns[cols])))\n",
    "    zz = stats.zscore(df.iloc[:,cols], nan_policy = 'omit')\n",
    "    zz.columns = list('Zscore_'+df.columns[cols])        \n",
    "    df = df.join(zz)\n",
    "  \n",
    "    #get median of each group  \n",
    "    group = {}\n",
    "    for s in range(len(samples)):\n",
    "        col = []\n",
    "        for i in range(len(list(df.columns))):\n",
    "            if ((df.columns[i].find('Zscore_')==0)&(df.columns[i].find('_'+samples[s])==(len(df.columns[i])-len(samples[s])-1))):\n",
    "                col.append(i)\n",
    "            group.update({samples[s]:col})\n",
    "    for i in range(len(group)):\n",
    "        df['Median_Z-score_'+list(group.keys())[i]] = df.iloc[:,(list(group.values())[i])].median(1)\n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def formating(df):\n",
    "    df['Gene name'] = [df['Gene Names'][i].split(';')[0] if type(df['Gene Names'][i])==str else '' for i in range(len(df)) ]\n",
    "    df['UniProt ID single'] = [df['Proteins'][i].split(';')[0] if type(df['Proteins'][i])==str else '' for i in range(len(df)) ]\n",
    "    df['Protein_Identifier'] = [str(df['Gene name'][i])+'_'+str(df['Proteins'][i])for i in range(len(df)) ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying in 3s\n",
      "Retrying in 3s\n",
      "Retrying in 3s\n",
      "Retrying in 3s\n"
     ]
    }
   ],
   "source": [
    "#path = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))+'\\\\Tables\\\\'\n",
    "path = os.path.dirname(os.path.abspath('__file__'))+'\\\\MQ_output\\\\' ## might be adjusted\n",
    "\n",
    "#with open('Z:/Fynn/005_Mito/Reanalysis/Mito_tissue atlas\\Mito_phos_fullprot_tissue/2_Tables_for_Website/Phos_colums.txt') as f:\n",
    "#    columns_to_import = [line.rstrip() for line in f]\n",
    "\n",
    "df = pd.read_csv((path+ 'modificationSpecificPeptides.txt'),sep = '\\t', low_memory=False)\n",
    "df = df[(df['Reverse']!='+') & (df['Potential contaminant']!='+')&(df['Phospho (STY)']>0)].reset_index(drop= True)\n",
    "flat_list = list(set([x for xs in [df['Proteins'][i].split(';') for i in range(len(df))] for x in xs]))\n",
    "ensemble_dict = map_UniProtID(flat_list,'UniProtKB_AC-ID','Ensembl')\n",
    "df['ESNG'] = [list(set([x for xs in [re.findall(r\"ENSMUSG\\d+\",ensemble_dict[item]) for item in df['Proteins'][i].split(';') if item in ensemble_dict.keys()] for x in xs])) for i in range(len(df))]\n",
    "entrez_dict = map_UniProtID(flat_list,'UniProtKB_AC-ID','GeneID')\n",
    "df['Entrez_GeneID'] = [list(set([entrez_dict[item] for item in df['Proteins'][i].split(';') if item in entrez_dict.keys()])) for i in range(len(df))]\n",
    "df = get_IMPI_annotation_PTM(df)\n",
    "df = get_MitoCharta_annotation(df)\n",
    "df = filter_Mito(df)\n",
    "df = filter_PTM(df,samples=['B','BAT','H','K','L','S','SKM'])\n",
    "\n",
    "path_saving = os.path.dirname(os.path.abspath('__file__'))+'\\\\Prepared_tables\\\\'\n",
    "df.to_csv((path_saving+ 'Prep_ModPeptides.csv'), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.8 (Mito_eLife)",
   "language": "python",
   "name": "mito_elife"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
